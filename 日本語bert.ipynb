{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "日本語bert",
      "provenance": [],
      "authorship_tag": "ABX9TyNkmj+rRzZZ7m2YYwxoBM1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydaigo/transformers/blob/master/%E6%97%A5%E6%9C%AC%E8%AA%9Ebert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxudzmolf7eO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "cb63161d-11c6-42c6-990d-3905334fc718"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "!git clone https://github.com/ydaigo/transformers.git\n",
        "%cd /content/transformers\n",
        "!pip install transformers\n",
        "!pip install mecab-python3\n",
        "!git pull\n",
        "!pip install --upgrade ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 23605 (delta 39), reused 80 (delta 19), pack-reused 23498\u001b[K\n",
            "Receiving objects: 100% (23605/23605), 14.04 MiB | 22.79 MiB/s, done.\n",
            "Resolving deltas: 100% (16701/16701), done.\n",
            "/content/transformers\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 3.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 15.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.33)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 29.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=f3eee977fd9fa11f637d4ab7ab04c2d535745837448111e0b5230450edac45a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 239kB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.5\n",
            "Already up to date.\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (1.12.33)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (0.1.85)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (0.0.38)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.7.0) (1.15.33)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.7.0) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.7.0) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.7.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.7.0) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.7.0) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.7.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.7.0) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.7.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.7.0) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers==2.7.0) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers==2.7.0) (2.8.1)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.7.0-cp36-none-any.whl size=557706 sha256=3b826c60771336cda4bf18c44ada6bab6141c5086e30584616c84a663e4a4f33\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rg_51pxl/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 2.7.0\n",
            "    Uninstalling transformers-2.7.0:\n",
            "      Successfully uninstalled transformers-2.7.0\n",
            "Successfully installed transformers-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeU3zu6ogamN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc474d1d-4e91-4a52-a8da-93c07011e37e"
      },
      "source": [
        "!python examples/run_glue.py \\\n",
        "    --data_dir=data_dir/ \\\n",
        "    --model_type=bert \\\n",
        "    --model_name_or_path=bert-base-japanese-whole-word-masking \\\n",
        "    --task_name=original \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir=output_dir/ \\"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 14:09:42.432037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/05/2020 14:09:44 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/05/2020 14:09:44 - INFO - filelock -   Lock 139980722468400 acquired on /root/.cache/torch/transformers/3c7c1cf64fda50b267a0e88bd23a88a7eaae4e8205f6ee4ceb472ef9e3274f29.7744b604e909ec6ffa1692fbdedb5b573f6348b1346d71aa16441b6956ef8f8a.lock\n",
            "04/05/2020 14:09:44 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpe1328w5l\n",
            "Downloading: 100% 383/383 [00:00<00:00, 286kB/s]\n",
            "04/05/2020 14:09:45 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json in cache at /root/.cache/torch/transformers/3c7c1cf64fda50b267a0e88bd23a88a7eaae4e8205f6ee4ceb472ef9e3274f29.7744b604e909ec6ffa1692fbdedb5b573f6348b1346d71aa16441b6956ef8f8a\n",
            "04/05/2020 14:09:45 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/3c7c1cf64fda50b267a0e88bd23a88a7eaae4e8205f6ee4ceb472ef9e3274f29.7744b604e909ec6ffa1692fbdedb5b573f6348b1346d71aa16441b6956ef8f8a\n",
            "04/05/2020 14:09:45 - INFO - filelock -   Lock 139980722468400 released on /root/.cache/torch/transformers/3c7c1cf64fda50b267a0e88bd23a88a7eaae4e8205f6ee4ceb472ef9e3274f29.7744b604e909ec6ffa1692fbdedb5b573f6348b1346d71aa16441b6956ef8f8a.lock\n",
            "04/05/2020 14:09:45 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json from cache at /root/.cache/torch/transformers/3c7c1cf64fda50b267a0e88bd23a88a7eaae4e8205f6ee4ceb472ef9e3274f29.7744b604e909ec6ffa1692fbdedb5b573f6348b1346d71aa16441b6956ef8f8a\n",
            "04/05/2020 14:09:45 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"original\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "04/05/2020 14:09:45 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json from cache at /root/.cache/torch/transformers/3c7c1cf64fda50b267a0e88bd23a88a7eaae4e8205f6ee4ceb472ef9e3274f29.7744b604e909ec6ffa1692fbdedb5b573f6348b1346d71aa16441b6956ef8f8a\n",
            "04/05/2020 14:09:45 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "04/05/2020 14:09:45 - INFO - filelock -   Lock 139980255972040 acquired on /root/.cache/torch/transformers/ae4d597c0697c617ab6c6c913effc265b7e606830a3b373dfcaeca1794ab9229.5fac9da4d8565963664ed9744688dc7008ff5ec4045f604e9515896f9fe46d9c.lock\n",
            "04/05/2020 14:09:45 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp7b411ah5\n",
            "Downloading: 100% 258k/258k [00:00<00:00, 2.18MB/s]\n",
            "04/05/2020 14:09:45 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-vocab.txt in cache at /root/.cache/torch/transformers/ae4d597c0697c617ab6c6c913effc265b7e606830a3b373dfcaeca1794ab9229.5fac9da4d8565963664ed9744688dc7008ff5ec4045f604e9515896f9fe46d9c\n",
            "04/05/2020 14:09:45 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/ae4d597c0697c617ab6c6c913effc265b7e606830a3b373dfcaeca1794ab9229.5fac9da4d8565963664ed9744688dc7008ff5ec4045f604e9515896f9fe46d9c\n",
            "04/05/2020 14:09:45 - INFO - filelock -   Lock 139980255972040 released on /root/.cache/torch/transformers/ae4d597c0697c617ab6c6c913effc265b7e606830a3b373dfcaeca1794ab9229.5fac9da4d8565963664ed9744688dc7008ff5ec4045f604e9515896f9fe46d9c.lock\n",
            "04/05/2020 14:09:45 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-vocab.txt from cache at /root/.cache/torch/transformers/ae4d597c0697c617ab6c6c913effc265b7e606830a3b373dfcaeca1794ab9229.5fac9da4d8565963664ed9744688dc7008ff5ec4045f604e9515896f9fe46d9c\n",
            "04/05/2020 14:09:45 - INFO - filelock -   Lock 139980255972656 acquired on /root/.cache/torch/transformers/a2f827453709c7a74b48468cfea4ca3c7ebc260308a4cd52d5231dd02b73e315.6225a45d8bbedb200842a8cef5bc97d0631dad427835b9e916dfeb1c3b2a91e8.lock\n",
            "04/05/2020 14:09:45 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp7twstto7\n",
            "Downloading: 100% 445M/445M [00:06<00:00, 71.3MB/s]\n",
            "04/05/2020 14:09:52 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-pytorch_model.bin in cache at /root/.cache/torch/transformers/a2f827453709c7a74b48468cfea4ca3c7ebc260308a4cd52d5231dd02b73e315.6225a45d8bbedb200842a8cef5bc97d0631dad427835b9e916dfeb1c3b2a91e8\n",
            "04/05/2020 14:09:52 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/a2f827453709c7a74b48468cfea4ca3c7ebc260308a4cd52d5231dd02b73e315.6225a45d8bbedb200842a8cef5bc97d0631dad427835b9e916dfeb1c3b2a91e8\n",
            "04/05/2020 14:09:52 - INFO - filelock -   Lock 139980255972656 released on /root/.cache/torch/transformers/a2f827453709c7a74b48468cfea4ca3c7ebc260308a4cd52d5231dd02b73e315.6225a45d8bbedb200842a8cef5bc97d0631dad427835b9e916dfeb1c3b2a91e8.lock\n",
            "04/05/2020 14:09:52 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-pytorch_model.bin from cache at /root/.cache/torch/transformers/a2f827453709c7a74b48468cfea4ca3c7ebc260308a4cd52d5231dd02b73e315.6225a45d8bbedb200842a8cef5bc97d0631dad427835b9e916dfeb1c3b2a91e8\n",
            "04/05/2020 14:09:55 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "04/05/2020 14:09:55 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "04/05/2020 14:09:55 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data_dir/', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-japanese-whole-word-masking', model_type='bert', n_gpu=0, no_cuda=False, num_train_epochs=3.0, output_dir='output_dir/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', task_name='original', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "04/05/2020 14:09:55 - INFO - __main__ -   Creating features from dataset file at data_dir/\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   Writing example 0/4\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   guid: train-0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   input_ids: 2 9727 187 10 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   input_ids: 2 4613 187 10 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   input_ids: 2 27145 308 10 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   input_ids: 2 4326 8342 10 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:09:55 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:09:55 - INFO - __main__ -   Saving features into cached file data_dir/cached_train_bert-base-japanese-whole-word-masking_128_original\n",
            "04/05/2020 14:09:55 - INFO - __main__ -   ***** Running training *****\n",
            "04/05/2020 14:09:55 - INFO - __main__ -     Num examples = 4\n",
            "04/05/2020 14:09:55 - INFO - __main__ -     Num Epochs = 3\n",
            "04/05/2020 14:09:55 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
            "04/05/2020 14:09:55 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "04/05/2020 14:09:55 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/05/2020 14:09:55 - INFO - __main__ -     Total optimization steps = 3\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:08<00:00,  8.20s/it]\n",
            "Epoch:  33% 1/3 [00:08<00:16,  8.20s/it]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:07<00:00,  7.52s/it]\n",
            "Epoch:  67% 2/3 [00:15<00:07,  8.00s/it]\n",
            "Iteration:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100% 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch: 100% 3/3 [00:23<00:00,  7.69s/it]\n",
            "04/05/2020 14:10:18 - INFO - __main__ -    global_step = 3, average loss = 0.6163366536299387\n",
            "04/05/2020 14:10:18 - INFO - __main__ -   Saving model checkpoint to output_dir/\n",
            "04/05/2020 14:10:18 - INFO - transformers.configuration_utils -   Configuration saved in output_dir/config.json\n",
            "04/05/2020 14:10:19 - INFO - transformers.modeling_utils -   Model weights saved in output_dir/pytorch_model.bin\n",
            "04/05/2020 14:10:19 - INFO - transformers.configuration_utils -   loading configuration file output_dir/config.json\n",
            "04/05/2020 14:10:19 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"original\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "04/05/2020 14:10:19 - INFO - transformers.modeling_utils -   loading weights file output_dir/pytorch_model.bin\n",
            "04/05/2020 14:10:22 - INFO - transformers.configuration_utils -   loading configuration file output_dir/config.json\n",
            "04/05/2020 14:10:22 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"original\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   Model name 'output_dir/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'output_dir/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   Didn't find file output_dir/added_tokens.json. We won't load it.\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file output_dir/vocab.txt\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file output_dir/special_tokens_map.json\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file output_dir/tokenizer_config.json\n",
            "04/05/2020 14:10:22 - INFO - transformers.configuration_utils -   loading configuration file output_dir/config.json\n",
            "04/05/2020 14:10:22 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"original\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   Model name 'output_dir/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'output_dir/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   Didn't find file output_dir/added_tokens.json. We won't load it.\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file output_dir/vocab.txt\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file output_dir/special_tokens_map.json\n",
            "04/05/2020 14:10:22 - INFO - transformers.tokenization_utils -   loading file output_dir/tokenizer_config.json\n",
            "04/05/2020 14:10:22 - INFO - __main__ -   Evaluate the following checkpoints: ['output_dir/']\n",
            "04/05/2020 14:10:22 - INFO - transformers.configuration_utils -   loading configuration file output_dir/config.json\n",
            "04/05/2020 14:10:22 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"original\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "04/05/2020 14:10:22 - INFO - transformers.modeling_utils -   loading weights file output_dir/pytorch_model.bin\n",
            "04/05/2020 14:10:25 - INFO - __main__ -   Creating features from dataset file at data_dir/\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   Writing example 0/2\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   input_ids: 2 1209 3545 5160 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   input_ids: 2 7216 23833 28447 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:10:25 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:10:25 - INFO - __main__ -   Saving features into cached file data_dir/cached_dev_bert-base-japanese-whole-word-masking_128_original\n",
            "04/05/2020 14:10:25 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "04/05/2020 14:10:25 - INFO - __main__ -     Num examples = 2\n",
            "04/05/2020 14:10:25 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 1/1 [00:00<00:00,  1.19it/s]\n",
            "04/05/2020 14:10:26 - INFO - __main__ -   ***** Eval results  *****\n",
            "04/05/2020 14:10:26 - INFO - __main__ -     acc = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}